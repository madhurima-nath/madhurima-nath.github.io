<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="keywords" content="madhurima nath, machine learning, 
    data science, computational physics, 
    epidemic modeling, network reliability, AI/ML, genai, 
    diffusive processes, graph dynamical systems, physics">
    <meta name="author" content="Madhurima Nath">
    <meta name="description" content="Personal homepage of Dr. Madhurima Nath">
    
    <title>Madhurima Nath</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <header>
        <h1>Madhurima Nath</h1>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">Home</li></a>
            <li><a href="genaiProjects.html">GenAI Projects</a></li>
            <li>NLP Projects</li>
            <li><a href="research.html">Research</a></li>
            <li><a href="teaching.html">Teaching</a></li>
        </ul>
    </nav>

    <main>
              
        <h2 style="color: #0A65C1;;">
            Optimizing multi-billion dollars spend portfolio for 
            global procurement
        </h2>
        <p>
            The procurement group of
            one of world's largest CPG (consumer product goods) companies
            struggled to gain visibility across its $34B spend
            portfolio due to lack of
            a single source of truth and manual practices.
            Achieving a small percentage (~2-3%) efficiency gain in the 
            data collection, cleansing, consolidation and 
            reporting would result in savings of millions of dollars.
            In its current state, the data was
            spread across multiple disparate ERP
            (Enterprise Resource Planning) platforms,
            with no common
            definitions or taxonomy structure for key data objects,
            i.e., product hierarchy, spend categories, supplier
            family etc., and weeks long exercise of manual 
            data collection and consolidation from internal systems 
            and asking around external suppliers etc.
            <br>
            After an extensive discovery process, the team
            designed and implemented an end-to-end Azure
            Databricks delta lakehouse solution architecture:
            <li>
                metadata driven, reusable data ingestion, 
                cleansing and processing pipelines
            </li>
            <li>
                product taxonomy harmonization using
                NLP models with MLOps integrated pipelines
            </li>
            <li>
                a set of dashboards showing key metrics and
                KPIs (key performance indicators)
            </li>
            <li>
                DevOps CI/CD framework to deploy the solution
            </li>
            This accelerated the data foundation roadmap by over a year, 
            delivered improved visibility, uniformity, and 
            consistency, and resulting in significant efficiency 
            gains.
        </p>

        <b style="color: blue;">
            Utility (To be completed)
        </b>
        <p>
        Directed the implementation of NLP 
        pipelines in Azure Databricks for sentiment analysis 
        and topic modeling to extract safety-related themes. 
        Empowered the executive safety committee of a large 
        Midwest utility client to refine policies, ensuring 
        employee safety and reducing incident frequency and severity.
        </p>

    </main>