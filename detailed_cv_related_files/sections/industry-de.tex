

{\textbf{Data Lake Implementation on Snowflake (12 months) $\|$ Nonprofit Organisation}}

\begin{zitemize}
 \item Maintained and upgraded Snowflake data lake processing 2M daily JSON bundles of statewide patient health information from 6 regional health information exchange networks.

\item Built Snowflake task orchestrations and parameterised stored procedures for batch ingestion and export pipelines, ensuring compliance with data protection regulations for sensitive patient information.

\item Refactored legacy monolithic JavaScript into modular Python and SQL components, improving maintainability and code quality.

\end{zitemize}

{\textbf{Network Predictive Maintenance Solution (4 weeks) $\|$ Telecom Client}}

\begin{zitemize}
\item Developed statistical analysis combining 6 RF performance metrics with geographical clustering to proactively identify faulty network infrastructure from user device data.

\item Implemented automated stored procedures on Google BigQuery processing 90-day network performance data across New York metropolitan area, collaborating with network engineers to validate preventive maintenance insights.

\item Projected $\sim$US \$8M annual operational cost savings through reduction of approximately 30K customer support calls and 6K service visits, with methodology validated for nationwide scaling.

\end{zitemize}

{\textbf{Data Migration to Salesforce (5 months) $\|$ Nonprofit Organisation}}
\begin{zitemize}
\item Migrated 1TB of historical data to Salesforce through custom SQL stored procedures, maintaining foreign key relationships across interdependent datasets ranging from hundreds of thousands to billions of rows.

\item Configured parameterised Azure Data Factory pipelines supporting full historical loads and incremental delta updates with table partitioning for large-scale dataset performance.

\item Built validation framework including row count verification, source-to-target comparison, and duplicate detection to ensure data integrity across migrations.

\end{zitemize}
